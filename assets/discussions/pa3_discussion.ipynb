{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Worker + Torch Distributed\n",
    "\n",
    "Ray worker keeps the state of the torch distributed process group. We provide a skeleton code to setup the processes to implement collective communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Print all Ray logs.\n",
    "os.environ['RAY_DEDUP_LOGS'] = '0'\n",
    "\n",
    "# Start Ray (locally)\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Worker:\n",
    "    def __init__(self, rank, world_size):\n",
    "        os.environ['RANK'] = str(rank)\n",
    "        os.environ['WORLD_SIZE'] = str(world_size)\n",
    "        os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        \n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.tensor = torch.tensor([float(self.rank + 1)])\n",
    "        print(f\"Rank {self.rank} initialized.\")\n",
    "    \n",
    "    def setup_torch_distributed(self):\n",
    "        dist.init_process_group(backend=\"gloo\", rank=self.rank, world_size=self.world_size)\n",
    "        return\n",
    "\n",
    "    def compute(self):\n",
    "        print(f\"[Before all_reduce] Rank {self.rank} tensor = {self.tensor.item()}\")\n",
    "        dist.all_reduce(self.tensor, op=dist.ReduceOp.SUM)\n",
    "        print(f\"[After all_reduce] Rank {self.rank} tensor = {self.tensor.item()}\")\n",
    "        return self.tensor.item()\n",
    "\n",
    "    def get_tensor(self):\n",
    "        return self.tensor.item()\n",
    "\n",
    "    def shutdown(self):\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "# Launch 4 actors\n",
    "world_size = 4\n",
    "workers = [Worker.remote(rank=i, world_size=world_size) for i in range(world_size)]\n",
    "\n",
    "# Trigger all_reduce\n",
    "ray.get([w.compute.remote() for w in workers])\n",
    "\n",
    "# Confirm the tensors\n",
    "tensors = ray.get([w.get_tensor.remote() for w in workers])\n",
    "print(\"Final tensors from all workers:\", tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleanup\n",
    "ray.get([w.shutdown.remote() for w in workers])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
