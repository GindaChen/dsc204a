---
title: Week 10
class: DSC204A
status: Active
---

Classes
: {: .label} ML Systems
  : [Slides]()
  
: {: .label} Guest Lecture by Zhijian Liu (NVIDIA) on Sparsity and Quantization

: {: .label} Wrap up

: *Optional Reading - No submission*
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)


  
<!--Class 1
: {: .label} ML System - 2
<!--  : [Slides](assets/slides/22_ml-system-2.pdf) &#8226; [Recording](https://podcast.ucsd.edu/watch/wi24/dsc204a_a00/26) &#8226; [Scribe Notes](assets/scribe_notes/Mar_11_scribe_note.pdf) 
: *Reading:*
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)



Class 2
: {: .label} ML System - 3
 <!-- : [Slides](assets/slides/23_ml-system-3.pdf) &#8226; [Recording](https://podcast.ucsd.edu/watch/wi24/dsc204a_a00/27) &#8226; [Scribe Notes](#) 
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)



Class 3
: {: .label} ML System - 4
: *Reading:* 
* [Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning (optional)](https://arxiv.org/pdf/2201.12023.pdf)
* [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (optional)](https://arxiv.org/pdf/1811.06965.pdf)
* [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (optional)](https://arxiv.org/pdf/1909.08053.pdf)
* [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (optional)](https://arxiv.org/pdf/2205.14135.pdf)
* [Efficient Memory Management for Large Language Model Serving with PagedAttention (optional)](https://arxiv.org/pdf/2309.06180.pdf)

-->
